# -*- coding: utf-8 -*-
"""caps_arbol_modelo.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ygEf2Q5stMlaHR87qwyZg6iXhmhtylsq
"""

import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix
import seaborn as sns

data =  pd.read_json('/content/sample_data/dataset.json')

# Separate the characteristics (X) and the target variable (y)
X = data.drop('Indicator', axis=1)
y = data['Indicator']

# Standardize features if necessary
scaler = StandardScaler()
X = scaler.fit_transform(X)

# The data in training (80%) and test (20%) sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create and train the decision tree model
model = DecisionTreeClassifier(random_state=42)
model.fit(X_train, y_train)

# Add new user features
new_answers = [2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1,2, 1, 1 ]
# Standardize the new user's responses
new_answers= scaler.transform([new_answers])

# Standardize the new user's responses
new_user_prediction = model.predict(new_answers)
print(f'Prediction for the new user: {new_user_prediction[0]}')

# Evaluate the model with the test data set
precision = model.score(X_test, y_test)
print(f'Precisión del modelo en el conjunto de prueba: {precision}')

# Realiza las predicciones en el conjunto de prueba
y_pred = model.predict(X_test)

# Generación de la matriz de confusión
conf_matrix = confusion_matrix(y_test, y_pred)

# Visualización de la matriz de confusión
plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')
plt.title('Matriz de Confusión')
plt.xlabel('Predicciones')
plt.ylabel('Valores Reales')
plt.show()

# Calcular métricas de rendimiento adicionales
accuracy = accuracy_score(y_test, y_pred)
precision_weighted = precision_score(y_test, y_pred, average='weighted')
recall_weighted = recall_score(y_test, y_pred, average='weighted')
f1_weighted = f1_score(y_test, y_pred, average='weighted')

# Mostrar las métricas de rendimiento
print(f'Accuracy: {accuracy}')
print(f'Precision (Weighted): {precision_weighted}')
print(f'Recall (Weighted): {recall_weighted}')
print(f'F1 Score (Weighted): {f1_weighted}')

